{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Linear Regression Analysis\n",
    "\n",
    "This notebook performs linear regression analysis on CPU performance data, comparing Simple Linear Regression (SLR), Multiple Linear Regression (MLR), and Recursive Feature Elimination (RFE).\n",
    "\n",
    "Linear regression is a linear approach to modelling the relationship between a continuous outcome variable and one or more continuous predictors/features. Although this is beyond the scope of this unit, linear regression can be applied to model non-linear relationships as well with clever manipulation of the predictors. The term “linear” only applies to the parameter coefficients, and not the predictors in the model.\n",
    "Applications of linear regression typically fall into two categories:\n",
    "\n",
    "(1)\tPrediction - model is used to predict the outcome based on some unobserved or untested values of the predictors.\n",
    "(2)\tAssociation - model estimates is used to explain the strength and nature of the relationship between the outcome and the predictors.\n",
    "\n",
    "In machine learning, application (1) is the main focus, while (2) is secondary or of little interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Install and Import Required Libraries\n",
    "\n",
    "To start, you need to install and load the relevant packages for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load and Examine CPU Data\n",
    "\n",
    "The CPU.csv data file contains performance data relating to computer hardware from various vendors and models. The variables in the files are as follows:\n",
    "\n",
    "(1)\tVendor name: 27 (adviser, amdahl, apollo, basf, bti, burroughs, c.r.d, cambex, cdc, dec, dg, formation, four-phase, gould, harris, honeywell, hp, ibm, ipl, magnuson, microdata, nas, ncr, nixdorf, perkin-elmer, prime, siemens)\n",
    "\n",
    "(2)\tModel name: many unique symbols\n",
    "\n",
    "(3)\tMYCT: machine cycle time in nanoseconds (integer)\n",
    "\n",
    "(4)\tMMIN: minimum main memory in kilobytes (integer)\n",
    "\n",
    "(5)\tMMAX: maximum main memory in kilobytes (integer)\n",
    "\n",
    "(6)\tCACH: cache memory in kilobytes (integer)\n",
    "\n",
    "(7)\tCHMIN: minimum channels in units (integer)\n",
    "\n",
    "(8)\tCHMAX: maximum channels in units (integer)\n",
    "\n",
    "(9)\tPRP: published relative CPU performance (integer)\n",
    "\n",
    "The goal here is to predict relative CPU performance based on its cycle time, memory and etc. The vendors’ names and the hardware model, i.e. Variables (1) and (2) are inconsequential in this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "url = \"https://raw.githubusercontent.com/asim-cv/MAT6206/refs/heads/main/Datasets/CPU.csv\"\n",
    "CPU = pd.read_csv(url)\n",
    "\n",
    "print(\"Data shape:\", CPU.shape)\n",
    "print(\"\\nData types:\")\n",
    "print(CPU.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(CPU.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create Training and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that PRP is the outcome of interest and that it is a numeric variable, linear regression modelling is therefore appropriate here. Not only do we want to build a linear regression model here, we also want to be evaluate the predictions from said model.\n",
    "\n",
    "Hence, we will now split the dataset into training and test sets. The training set will be used to build the model, and the test set will be used to evaluate its predictive performance. Here, we will use a 75/25 split, i.e. randomly select 75% of the original data to be in the training set, and the remaining 25% form the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split the data into training (75%) and test (25%) sets\n",
    "trainData, testData = train_test_split(CPU, test_size=0.25, random_state=1)\n",
    "\n",
    "print(f\"Training set size: {len(trainData)}\")\n",
    "print(f\"Test set size: {len(testData)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Simple Linear Regression (SLR) - PRP vs MMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression refers to the situation where the outcome variable is regressed against one predictor or feature.\n",
    "\n",
    "Suppose, at this moment, we are only interested in predicting PRP from maximum main memory, i.e. MMAX. Let us examine their relationship via a scatter plot and Person’s correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PRP and MMAX from training data\n",
    "trainData_slr = trainData[['PRP', 'MMAX']]\n",
    "\n",
    "# Plot PRP against MMAX\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(trainData_slr['MMAX'], trainData_slr['PRP'], \n",
    "           color='red', alpha=0.5, s=50)\n",
    "plt.xlabel('MMAX')\n",
    "plt.ylabel('PRP')\n",
    "plt.title('PRP vs MMAX')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Pearson's correlation coefficient\n",
    "correlation = trainData_slr.corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Simple Linear Regression model\n",
    "X_slr = trainData_slr[['MMAX']]\n",
    "y_slr = trainData_slr['PRP']\n",
    "\n",
    "mod_slr = LinearRegression()\n",
    "mod_slr.fit(X_slr, y_slr)\n",
    "\n",
    "# Model summary using statsmodels for detailed statistics\n",
    "X_slr_sm = sm.add_constant(X_slr)\n",
    "mod_slr_sm = sm.OLS(y_slr, X_slr_sm).fit()\n",
    "\n",
    "print(\"Simple Linear Regression Summary:\")\n",
    "print(mod_slr_sm.summary())\n",
    "\n",
    "# Show estimated coefficients\n",
    "print(f\"\\nIntercept: {mod_slr.intercept_:.3f}\")\n",
    "print(f\"MMAX coefficient: {mod_slr.coef_[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the predictive performance of this model on the test data and determine the prediction root mean squared error (RMSE) and bias, and the correlation and predicted R2 between the actual and predicted PRP values for the test set. Prediction RMSE and bias that are close to zero, and a correlation value that is close to one are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PRP with test data\n",
    "X_test_slr = testData[['MMAX']]\n",
    "pred_slr = mod_slr.predict(X_test_slr)\n",
    "\n",
    "# Calculate performance metrics\n",
    "diff_slr = testData['PRP'] - pred_slr\n",
    "RMSE_slr = np.sqrt(mean_squared_error(testData['PRP'], pred_slr))\n",
    "bias_slr = np.mean(diff_slr)\n",
    "cor_slr = np.corrcoef(testData['PRP'], pred_slr)[0, 1]\n",
    "predR2_slr = cor_slr**2\n",
    "\n",
    "# Show results for SLR\n",
    "SLR_pf = {\n",
    "    'RMSE': RMSE_slr,\n",
    "    'Bias': bias_slr,\n",
    "    'Corr': cor_slr,\n",
    "    'PredR2': predR2_slr\n",
    "}\n",
    "\n",
    "print(\"Simple Linear Regression Performance:\")\n",
    "for metric, value in SLR_pf.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for SLR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pred_slr, testData['PRP'], color='steelblue', s=50)\n",
    "plt.plot([0, 500], [0, 500], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Predicted PRP')\n",
    "plt.ylabel('Actual PRP')\n",
    "plt.title('Actual vs Predicted PRP (SLR)')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Multiple Linear Regression (MLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLR, the continuous outcome is linearly modelled as a function of two or more continuous predictors or features.\n",
    "\n",
    "We will revisit the CPU data, but this time we will consider all six predictors, i.e. Columns 3 to 8. \n",
    "\n",
    "First, we will generate a scatter plot matrix between all the predictors and the outcome PRP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairplot for numerical variables (lower triangle only)\n",
    "numerical_cols = trainData.select_dtypes(include=[np.number]).columns\n",
    "# Use corner=True to explicitly show only lower triangle\n",
    "sns.pairplot(trainData[numerical_cols], diag_kind='hist', corner=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row of the scatter plot matrix shows the bivariate relationship between PRP with each of the 6 features. It is clear from the 1st scatter plot along this row that PRP is not linearly correlated with MYCT and that transforming the data may be useful here. In fact, the two are actually log-linearly related, but let’s not worry about this. If we leave MYCT as is in the MLR model, then it is likely that the final result will shown that MYCT is not a significant predictor of PRP. However, it may not be wise to conclude which variables are NOT important based solely on these plots. In particular for variables with uninteresting pattern that sometimes they can help explain certain aspects of the outcome variable that the visually important variables may not.\n",
    "\n",
    "With the scatter plot matrix, it is also important to observe the relationship between the features, and in particular, those that are highly correlated with each other as this can imply collinearity in our data. We can dig deeper by examining the correlation matrix between the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix heatmap (excluding PRP column)\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Select only predictor variables (exclude PRP)\n",
    "predictor_cols = trainData[numerical_cols].drop('PRP', axis=1)\n",
    "correlation_matrix = predictor_cols.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "            center=0, square=True, fmt='.3f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Building MLR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go ahead and build the multiple regression model and regress PRP against the 6 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for MLR (excluding non-numeric columns)\n",
    "feature_cols = ['MYCT', 'MMIN', 'MMAX', 'CACH', 'CHMIN', 'CHMAX']\n",
    "X_mlr = trainData[feature_cols]\n",
    "y_mlr = trainData['PRP']\n",
    "\n",
    "# Fit Multiple Linear Regression model\n",
    "mod_mlr = LinearRegression()\n",
    "mod_mlr.fit(X_mlr, y_mlr)\n",
    "\n",
    "# Model summary using statsmodels\n",
    "X_mlr_sm = sm.add_constant(X_mlr)\n",
    "mod_mlr_sm = sm.OLS(y_mlr, X_mlr_sm).fit()\n",
    "\n",
    "print(\"Multiple Linear Regression Summary:\")\n",
    "print(mod_mlr_sm.summary())\n",
    "\n",
    "# Show estimated coefficients\n",
    "print(f\"\\nIntercept: {mod_mlr.intercept_:.3f}\")\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    print(f\"{feature} coefficient: {mod_mlr.coef_[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance inflation factor (VIF) can be generated. Is there any evidence of collinearity based on the VIF measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for multicollinearity check\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_results = calculate_vif(X_mlr)\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Evaluating the MLR Model\n",
    "\n",
    "We will now evaluate the predictive performance of the MLR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PRP with test data\n",
    "X_test_mlr = testData[feature_cols]\n",
    "pred_mlr = mod_mlr.predict(X_test_mlr)\n",
    "\n",
    "# Calculate performance metrics\n",
    "diff_mlr = testData['PRP'] - pred_mlr\n",
    "RMSE_mlr = np.sqrt(mean_squared_error(testData['PRP'], pred_mlr))\n",
    "bias_mlr = np.mean(diff_mlr)\n",
    "cor_mlr = np.corrcoef(testData['PRP'], pred_mlr)[0, 1]\n",
    "predR2_mlr = cor_mlr**2\n",
    "\n",
    "# Predictive measures for MLR\n",
    "MLR_pf = {\n",
    "    'RMSE': RMSE_mlr,\n",
    "    'Bias': bias_mlr,\n",
    "    'Corr': cor_mlr,\n",
    "    'PredR2': predR2_mlr\n",
    "}\n",
    "\n",
    "print(\"Multiple Linear Regression Performance:\")\n",
    "for metric, value in MLR_pf.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for MLR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pred_mlr, testData['PRP'], color='steelblue', s=50)\n",
    "plt.plot([0, 500], [0, 500], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Predicted PRP')\n",
    "plt.ylabel('Actual PRP')\n",
    "plt.title('Actual vs Predicted PRP (MLR)')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SLR and MLR results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'SLR': list(SLR_pf.values()),\n",
    "    'MLR': list(MLR_pf.values())\n",
    "}, index=list(SLR_pf.keys()))\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to overcome or alleviate this problem, including performing the log-likelihood test, stepwise regression or other variable selection methods. Here we will perform a recursive feature elimination (RFE). The RFE, a backward variable selection process, is as follows:\n",
    "\n",
    "Step 1: Build a model to all features in the training set and rank each feature based on its importance to the model. In our case, it is the p-value.\n",
    "\n",
    "Step 2: Keeping priority to the most important variables, iterate through by building models of given subset sizes, that is, subsets of most important predictors determined from step 1. Ranking of the features is recalculated in each iteration.\n",
    "\n",
    "Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination (RFE) with Cross Validation\n",
    "from random import shuffle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Define the subsets of features to test (equivalent to R's subsets <- c(2:6))\n",
    "subsets = list(range(2, 7))  # [2, 3, 4, 5, 6] features\n",
    "\n",
    "print(\"Testing feature subsets:\", subsets)\n",
    "\n",
    "# Perform RFE with cross-validation (equivalent to R's rfeControl with repeatedcv)\n",
    "estimator = LinearRegression()\n",
    "cv = KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "\n",
    "rfe_cv = RFECV(estimator=estimator, \n",
    "               step=1,  # Remove one feature at a time\n",
    "               cv=cv,   # 10-fold cross validation\n",
    "               scoring='neg_mean_squared_error',  # Use MSE as scoring metric\n",
    "               min_features_to_select=2)  # Minimum 2 features\n",
    "\n",
    "# Fit RFE with features only (columns 3-8, excluding PRP)\n",
    "feature_cols = ['MYCT', 'MMIN', 'MMAX', 'CACH', 'CHMIN', 'CHMAX']\n",
    "X_rfe = trainData[feature_cols]\n",
    "y_rfe = trainData['PRP']\n",
    "\n",
    "rfe_cv.fit(X_rfe, y_rfe)\n",
    "\n",
    "print(\"RFE Results:\")\n",
    "print(f\"Optimal number of features: {rfe_cv.n_features_}\")\n",
    "print(f\"Selected features: {[feature_cols[i] for i in range(len(feature_cols)) if rfe_cv.support_[i]]}\")\n",
    "print(f\"Feature rankings: {dict(zip(feature_cols, rfe_cv.ranking_))}\")\n",
    "print(f\"Cross-validation scores: {rfe_cv.cv_results_['mean_test_score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, the RFE algorithm suggests that all 6 features should be retained from a prediction accuracy standpoint, even though, statistically speaking, both CHMAX and MYCT are non-significant predictors.\n",
    "\n",
    "Note that at the end of the RFE process, only the optimal model is stored, and as one of the components (called fit) in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the optimal model and show its coefficients\n",
    "optimal_model = rfe_cv.estimator_\n",
    "\n",
    "print(\"Optimal Model Coefficients:\")\n",
    "print(f\"Intercept: {optimal_model.intercept_:.3f}\")\n",
    "\n",
    "selected_features = [feature_cols[i] for i in range(len(feature_cols)) if rfe_cv.support_[i]]\n",
    "for i, feature in enumerate(selected_features):\n",
    "    print(f\"{feature}: {optimal_model.coef_[i]:.3f}\")\n",
    "\n",
    "# Model summary using statsmodels\n",
    "X_optimal = X_rfe.iloc[:, rfe_cv.support_]\n",
    "X_optimal_sm = sm.add_constant(X_optimal)\n",
    "optimal_model_sm = sm.OLS(y_rfe, X_optimal_sm).fit()\n",
    "\n",
    "print(\"\\nOptimal Model Summary:\")\n",
    "print(optimal_model_sm.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the optimal model and predict on the test set, we can use the predict(.) function as per usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set using the optimal model\n",
    "X_test_optimal = testData[selected_features]\n",
    "pred_optimal = optimal_model.predict(X_test_optimal)\n",
    "\n",
    "#print(\"Predictions (first 10):\")\n",
    "#print(pred_optimal[:10].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics for RFE model\n",
    "diff_rfe = testData['PRP'] - pred_optimal\n",
    "RMSE_rfe = np.sqrt(mean_squared_error(testData['PRP'], pred_optimal))\n",
    "bias_rfe = np.mean(diff_rfe)\n",
    "cor_rfe = np.corrcoef(testData['PRP'], pred_optimal)[0, 1]\n",
    "predR2_rfe = cor_rfe**2\n",
    "\n",
    "# Show results for RFE\n",
    "RFE_pf = {\n",
    "    'RMSE': RMSE_rfe,\n",
    "    'Bias': bias_rfe,\n",
    "    'Corr': cor_rfe,\n",
    "    'PredR2': predR2_rfe\n",
    "}\n",
    "\n",
    "print(\"RFE Model Performance:\")\n",
    "for metric, value in RFE_pf.items():\n",
    "    print(f\"{metric}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross Validation Comparison\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "\n",
    "# Set up 10-fold repeated cross validation (equivalent to R's repeatedcv with repeats=10)\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "# Cross validation for SLR\n",
    "slr_cv_scores = cross_val_score(mod_slr, X_slr, y_slr, \n",
    "                               cv=rkf, scoring='neg_mean_squared_error')\n",
    "slr_cv_rmse = np.sqrt(-slr_cv_scores)\n",
    "\n",
    "# Cross validation for MLR\n",
    "mlr_cv_scores = cross_val_score(mod_mlr, X_mlr, y_mlr, \n",
    "                               cv=rkf, scoring='neg_mean_squared_error')\n",
    "mlr_cv_rmse = np.sqrt(-mlr_cv_scores)\n",
    "\n",
    "# Cross validation for RFE optimal model\n",
    "rfe_cv_scores = cross_val_score(optimal_model, X_optimal, y_rfe, \n",
    "                               cv=rkf, scoring='neg_mean_squared_error')\n",
    "rfe_cv_rmse = np.sqrt(-rfe_cv_scores)\n",
    "\n",
    "print(\"10-Fold Cross Validation Results (RMSE):\")\n",
    "print(f\"SLR: {slr_cv_rmse.mean():.3f} ± {slr_cv_rmse.std():.3f}\")\n",
    "print(f\"MLR: {mlr_cv_rmse.mean():.3f} ± {mlr_cv_rmse.std():.3f}\")\n",
    "print(f\"RFE: {rfe_cv_rmse.mean():.3f} ± {rfe_cv_rmse.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Comparison - All Models\n",
    "final_comparison = pd.DataFrame({\n",
    "    'SLR': list(SLR_pf.values()),\n",
    "    'MLR': list(MLR_pf.values()),\n",
    "    'RFE': list(RFE_pf.values())\n",
    "}, index=list(SLR_pf.keys()))\n",
    "\n",
    "print(\"Final Model Comparison:\")\n",
    "print(final_comparison.round(3))\n",
    "\n",
    "# Add cross-validation results\n",
    "cv_comparison = pd.DataFrame({\n",
    "    'SLR_CV': [slr_cv_rmse.mean(), slr_cv_rmse.std()],\n",
    "    'MLR_CV': [mlr_cv_rmse.mean(), mlr_cv_rmse.std()],\n",
    "    'RFE_CV': [rfe_cv_rmse.mean(), rfe_cv_rmse.std()]\n",
    "}, index=['CV_RMSE_Mean', 'CV_RMSE_Std'])\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_comparison.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Cross-Validation Results Box Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create subplot for cross-validation results\n",
    "plt.subplot(2, 2, 1)\n",
    "cv_data = [slr_cv_rmse, mlr_cv_rmse, rfe_cv_rmse]\n",
    "plt.boxplot(cv_data, labels=['SLR', 'MLR', 'RFE'])\n",
    "plt.title('10-Fold Cross Validation RMSE Distribution')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted plots for all models\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(pred_slr, testData['PRP'], color='steelblue', s=50, alpha=0.7)\n",
    "plt.plot([0, 500], [0, 500], 'r--', linewidth=2)\n",
    "plt.xlabel('Predicted PRP')\n",
    "plt.ylabel('Actual PRP')\n",
    "plt.title('SLR: Actual vs Predicted')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(pred_mlr, testData['PRP'], color='steelblue', s=50, alpha=0.7)\n",
    "plt.plot([0, 500], [0, 500], 'r--', linewidth=2)\n",
    "plt.xlabel('Predicted PRP')\n",
    "plt.ylabel('Actual PRP')\n",
    "plt.title('MLR: Actual vs Predicted')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(pred_optimal, testData['PRP'], color='steelblue', s=50, alpha=0.7)\n",
    "plt.plot([0, 500], [0, 500], 'r--', linewidth=2)\n",
    "plt.xlabel('Predicted PRP')\n",
    "plt.ylabel('Actual PRP')\n",
    "plt.title('RFE: Actual vs Predicted')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
